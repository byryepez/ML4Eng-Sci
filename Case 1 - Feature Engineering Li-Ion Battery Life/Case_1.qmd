---
title: "Case 1: Feature Engineering"
author:  Prof. Richard Braatz


format:
    html: 
        theme: zephyr
        html-math-method: katex
        embed-resources: true
        self-contained-math: true
        number-sections: true
        title-block-banner: true
        grid: 
            body-width: 1000px

---

## Introduction


RICHARD BRAATZ: Hello. I'm Professor Richard Braatz, and I am affiliated with the Department of Chemical Engineering and the Center for Computational Science and Engineering at the Massachusetts Institute of Technology. These videos teach feature engineering and some related concepts which will be presented within the context of a specific application. The specific application will be lithium ion batteries and, in particular, the prediction of their cycle lifetimes. First, I'll provide some background on lithium ion batteries and lifetime prediction. Then, I'll describe the feature engineering approach to machine learning which constructs highly nonlinear models while having low computational cost. I'll define the term feature, and then I'll describe the tasks of feature engineering and feature selection. I'll define regularization and its use in improving model building. And then I'll apply feature engineering to build a model for lithium ion battery lifetime prediction.

## Predicting Battery lifetime

PROFESSOR: Lithium ion batteries are ubiquitous in modern society, and they appear in many consumer products, such as cell phones, laptops, and electric vehicles. But they also appear in satellites, and in implantable medical devices. The value of a battery in an application is directly related to the lifetime of the battery-- basically, the number of cycles before the battery is no longer useful for the particular application. An objective that's important for batteries is to use earlier cycling data to predict the overall battery lifetime. The challenge is that even for batteries with the same manufacturing line, battery lifetime has large variability. And that variability tends to increase over time. Two batteries can actually have very different lifetimes, even if they have the same charging and discharging cycles. So companies would like to be able to predict the battery lifetime for each battery based on the past data collected for the particular battery. And the more accurate and earlier prediction, the more valuable the information. The way it's collected is is using high throughput cyclers. And so a cycler will have a large number of batteries that are all operating in parallel. And for each of those batteries, the cycler will control the temperature of the battery, and will control the charge and discharge cycles. And the data is collected for each of those batteries. And that data is going to be able to be used for predicting the overall lifetime of each of the batteries, and determine which of the batteries are the long-lasting batteries, and which ones are the short-lasting batteries. And the goal in all of this is with a very small number of cycles-- because cycles are going to cost you energy; it costs you energy to charge and discharge batteries-- you're going to be able to try to predict-- or want to be able to predict-- what the long-term performance of that battery is going to be long into the future, which could be on the order of 1,000 or 2,000 cycles. Now, you might think that it might be pretty easy to be able to just predict that, because you could just take the capacity, or the amount of charge in the battery, and then just see if that amount of charge is changing slowly, or is roughly constant, or changing quickly, and trying to use that to predict how the capacity will change over the future. Now, many data-driven studies have definitely worked on trying to make those kind of predictions. And the way they've done it is is many have tried to do it based on discharge capacity. So what they'll do is they'll just run through discharges and see how much charge or capacity, how many electrons they got out of the battery. And then they'll try to collect that information, and either take the value of that information or the change in that information-- such as seeing if the capacity's going down quickly or shortly-- and use that to try to predict the overall capacity over the lifetime of the battery. It turns out that approach doesn't work. A battery can actually have fairly constant capacity, and then very quickly ramp down, and basically be a poor battery, whereas another battery can actually have the capacity reducing earlier, and then it becomes very flat and then lasts for a very long time. And so if you try to correlate the current capacity to the long-term capacity into the future, or if you try to correlate the change in capacity long into the future, all those correlations have very, very poor correlation. And that's why people have looked into using more advanced machine learning methods for building these kind of models.

## Identifying Features


INSTRUCTOR: Now let's discuss the future engineering approach to machine learning for constructing highly nonlinear models while having low computational cost and having high predictive accuracy. A feature is a transformation of the raw data in such a way that it's useful or potentially useful for some machine learning task. A classic example of a feature is to calculate the logarithm of a data point or an exponential of a data point. Well, you might take a few of the data points and multiply them together. You can define features using just these simple kind of nonlinear linear transformations. But it's also very useful, typically in applications, to define features based on the chemistry or the physics of the problem. For example, for lithium ion battery, it could be useful to define the feature in terms of the energy of the battery. But you don't actually directly measure in experiments the energy of the battery. You might measure-- and do measure-- the current or the voltage over time. And from that information, you can compute, using physics, what the energy is. And it's actually that energy that can be a useful feature. Once you defined all the features, you put them up into a vector. And that's typically called the feature vector. And you don't have to know which features are going to be the really good ones or really the bad ones. Propose a bunch of features that you think might be useful. And later on, we're going to have a procedure that's going to reduce the number of features later. And so you want to be able to pick features that could be useful, and try to have as much guesses on, well, this is probably useful, and that's probably useful, and this is probably useful. And then, maybe these are others that could be useful. And you just stack that up into a vector. And the key of using this kind of approach is that the model prediction will be defined as a linear combination of the features. Now, this approach is going to allow us to construct a nonlinear model, because we've incorporated all of the nonlinearity into the features. Now, you can also take nonlinear transformations of the output, as well. But the model that you're going to compute will be the nonlinear transform of the output, or the output directly, equals linear combinations of the features. And because it's linear combinations of the features, we're going to be able to use what's called linear regression. Now, linear regression is written in the form of an optimization problem. And basically, you're solving a minimization of the sum of squared errors. And because we have a linear model that's a linear combination of the features, we'll be able to do this linear regression very, very quickly using computers, or even sometimes, analytically. Now, in this case, the parameters are going to be regression coefficients that are in front of each of our features. And because the optimization is very fast, we can actually solve that for a very large number of features. We won't actually do that, though. Because in manufacturing or processed data sets, and in many applications, you can't actually solve the optimization for a large number of features. You can do it numerically, but you'll end up overfitting. And to avoid overfitting, we're going to use a technique known as regularization, which is described in the next video.

## Regularization

PROFESSOR: Regularization is a technique for producing more reliable models by incorporating a penalty on the regression coefficients into the objective function when doing the model-building procedure. You can think of regularization as really being in basically three different types. One type is known as ridge regression. Sometimes they also just call this the quadratic penalty. What you do is you take your sum of squared errors, which is normally what you're doing for developing a good model prediction. And you're adding a positive penalty term, which is just the square of the sum of the regression coefficients. This technique leads to much more accurate predictions, especially if you have a large number of features. A second technique is known as LASSO. What LASSO does is its penalty is the sum of the absolute value of the regression coefficients. And what this technique is very effective in doing is that if a regression coefficient isn't really needed or can be basically zero without affecting your model prediction, LASSO will just drive those regression coefficients to zero. And as each of those regression coefficients go to zero, the corresponding features also go to zero. And so you could start off with a large vector of features and apply it-- by applying LASSO, you can reduce the number of features to just those essential features that are needed for the model prediction. The third class of approach is basically within the class known as elastic net. And elastic net basically is a weighted combination of LASSO and ridge regression. So it has a quadratic penalty, which creates a lot of robustness and reliability. But it also has that LASSO character of just taking the features that are only the ones that are really essential for the model of prediction and only including those in the model. Over time, elastic net is used more and more relative to LASSO, because elastic net is much more reliable and more robust when you have perturbations in the data.

## Applying Feauture Engineering to Battery Lifetime Prediction


PROFESSOR: Here we describe the application of feature engineering to building a model for lifetime prediction for lithium ion batteries. Data were collected by a collaborator on a high throughput battery cycler. From that data, we then computed features that included all of the information that we thought could be predictive of lifetime. Features included the average temperature during cycling, the variability of temperature during cycling, because we know temperature's is important. But we also computed more complicated, nonlinear transformations of the data such as the energy of the battery at each cycle. From this information, we fed that into an elastic net procedure that then down selected the features to a small number of features. And the quadratic term of elastic net leads to a robustification or improved model predictive capability. And with these aspects of elastic net, we're able to develop models with a very small number of features. They were able to accurately predict the lifetime of the batteries. And we tested and validated this procedure for many different combinations of testing data and validation data for wide ranges of fast charging, to slow charging, to intermediate charging. And with that high quality of prediction that we're able to achieve, we then decided, could we take it to the next step? And could we actually use the same sort of features for classifying batteries in terms of long lasting and short lasting? This procedure was very effective. And not only were we able to classify batteries in terms of long and short lasting, but were able to do it in only five cycles before you could even measure any significant reduction in capacity of the batteries.

## Summary

RICHARD BRAATZ: These videos describe the feature engineering approach for constructing highly nonlinear models from limited data. The presentation was within the context of a real industrial problem, predicting lifetime of lithium ion batteries from early cycling data. Features, which are non-linear transformations of the raw data, were constructed based on conversations with battery experts. The elastic net employed regularization to down-select the features to a small number, and the method is generally more robust than LASSO. This machine learning method accurately predicted battery lifetimes and classified batteries from data collected only during the first five cycles. The ability to classify batteries early means that batteries can be sorted into bins, based on lifetime, shortly after being manufactured. Feature engineering and elastic net were applied to a specific application, but they have been applied to many industries and are generally useful for constructing nonlinear, predictive models from limited data.